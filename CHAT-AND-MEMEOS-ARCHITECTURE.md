# Chat with Archive + MemeOS Integration Architecture

**Date:** November 21, 2025
**Goal:** Build toward conversational archive search and personalized content recommendation from MemeOS

---

## Executive Summary

You have three powerful building blocks already in place:
1. **Personal Profile Generation** - LLM-analyzed summary of your writing style and interests
2. **Thread Export** - Structured export of all your writing
3. **Multi-Model LLM Infrastructure** - Async task processing with GPT and Claude

The next evolution connects these pieces into two complementary features:
- **Chat with Archive**: RAG-powered conversational search over your writing
- **MemeOS Integration**: Context-aware content recommendations from your bookmarks/Twitter

This document provides a phased architecture for building both features in a way that maximizes reuse and minimizes complexity.

---

## Current State: What You Have

### 1. Personal Profile (`UserProfile` model)
```python
# backend/models.py
class UserProfile(db.Model):
    user_id → which user
    content → LLM-generated or user-written profile
    generated_by → "user" or model ID
    tokens_used → cost tracking
    created_at → timestamp
```

**Generated by:** Analyzing up to 268.5k tokens of your writing
**Contains:** Personality traits, writing style, recurring themes, interests

### 2. Export Infrastructure
- `build_user_export_content()` - Formats all threads as hierarchical text
- Token-limited export (can select most recent N tokens of writing)
- Thread tree structure preserved (parent-child relationships)

### 3. Multi-Model LLM
- Provider abstraction (OpenAI, Anthropic)
- Async task processing (Celery)
- Progress tracking
- Error handling

---

## Phase 1: RAG Foundation (Chat with Archive)

### Goal
Enable semantic search and conversational queries over your writing archive.

**Use Cases:**
- "What have I written about consciousness?"
- "Show me my thoughts on building habits"
- "Summarize my entries from last month where I discussed AI"
- "Find the thread where I talked to Claude about meaning"

### Architecture Components

#### 1.1 Vector Embeddings

**Choice: PostgreSQL + pgvector extension**

Why pgvector:
- Already using PostgreSQL
- No additional infrastructure
- Excellent performance for <1M vectors
- Open source, no vendor lock-in
- Built-in hybrid search (semantic + keyword)

**Schema Addition:**
```sql
-- Migration: Add vector column to nodes
ALTER TABLE node ADD COLUMN embedding vector(1536);

-- Create index for fast similarity search
CREATE INDEX ON node USING ivfflat (embedding vector_cosine_ops);
```

**Alternative (if scaling becomes an issue):**
- Pinecone (managed, easy, $70/mo for hobbyist tier)
- Weaviate (self-hosted, feature-rich)
- Qdrant (modern, fast, self-hosted)

#### 1.2 Embedding Generation

**Model: OpenAI `text-embedding-3-small`**
- $0.02 per 1M tokens (very cheap)
- 1536 dimensions
- Fast and accurate
- Same provider as your LLM

**Implementation:**
```python
# backend/tasks/embeddings.py

from openai import OpenAI
from backend.celery_app import celery, flask_app
from backend.models import Node
from backend.extensions import db

@celery.task
def generate_embedding_for_node(node_id: int):
    """Generate and store embedding for a single node."""
    with flask_app.app_context():
        node = Node.query.get(node_id)
        if not node or not node.content:
            return

        client = OpenAI(api_key=flask_app.config["OPENAI_API_KEY"])

        # Generate embedding
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=node.content[:8000]  # Limit to ~8k chars
        )

        embedding = response.data[0].embedding

        # Store in database
        node.embedding = embedding
        db.session.commit()

        return {"node_id": node_id, "status": "embedded"}

@celery.task
def embed_all_nodes(user_id: int):
    """Batch embed all nodes for a user (run once for backfill)."""
    with flask_app.app_context():
        nodes = Node.query.filter_by(user_id=user_id).all()

        for node in nodes:
            if node.embedding is None:  # Skip already embedded
                generate_embedding_for_node.delay(node.id)

        return {"total_nodes": len(nodes), "status": "embedding_queued"}
```

**Trigger Points:**
- On node creation (automatic)
- On node edit (regenerate embedding)
- Backfill command for existing nodes

#### 1.3 Semantic Search

**Implementation:**
```python
# backend/routes/search.py

from flask import Blueprint, request, jsonify
from flask_login import login_required, current_user
from backend.models import Node
from backend.extensions import db
from openai import OpenAI

search_bp = Blueprint("search", __name__)

@search_bp.route("/semantic", methods=["POST"])
@login_required
def semantic_search():
    """
    Semantic search over user's nodes.

    Request:
        {
            "query": "What have I written about consciousness?",
            "limit": 10,
            "threshold": 0.7  // cosine similarity threshold
        }

    Returns:
        {
            "results": [
                {
                    "node_id": 123,
                    "content": "...",
                    "similarity": 0.89,
                    "created_at": "...",
                    "parent_id": 45  // for context
                }
            ]
        }
    """
    data = request.get_json()
    query = data.get("query")
    limit = data.get("limit", 10)
    threshold = data.get("threshold", 0.7)

    if not query:
        return jsonify({"error": "Query is required"}), 400

    # Generate embedding for query
    client = OpenAI(api_key=current_app.config["OPENAI_API_KEY"])
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=query
    )
    query_embedding = response.data[0].embedding

    # Search using pgvector
    results = db.session.execute(
        """
        SELECT
            id,
            content,
            created_at,
            parent_id,
            1 - (embedding <=> :query_embedding) as similarity
        FROM node
        WHERE
            user_id = :user_id
            AND embedding IS NOT NULL
            AND 1 - (embedding <=> :query_embedding) > :threshold
        ORDER BY similarity DESC
        LIMIT :limit
        """,
        {
            "query_embedding": str(query_embedding),
            "user_id": current_user.id,
            "threshold": threshold,
            "limit": limit
        }
    ).fetchall()

    return jsonify({
        "query": query,
        "results": [
            {
                "node_id": r.id,
                "content": r.content[:500],  # Preview
                "similarity": float(r.similarity),
                "created_at": r.created_at.isoformat(),
                "parent_id": r.parent_id
            }
            for r in results
        ]
    }), 200
```

#### 1.4 Chat Interface (RAG)

**Flow:**
1. User asks question
2. Generate embedding for question
3. Retrieve top-K most similar nodes (K=5-10)
4. Build context from retrieved nodes
5. Send to LLM with context + question
6. Return answer with citations

**Implementation:**
```python
# backend/routes/chat.py

from flask import Blueprint, request, jsonify
from flask_login import login_required, current_user
from backend.llm_providers import LLMProvider
from backend.routes.search import semantic_search_internal  # helper version

chat_bp = Blueprint("chat", __name__)

@chat_bp.route("/archive", methods=["POST"])
@login_required
def chat_with_archive():
    """
    Chat with your archive using RAG.

    Request:
        {
            "query": "What patterns do I see in my thinking about AI?",
            "model": "claude-sonnet-4.5",
            "context_size": 5  // number of nodes to retrieve
        }

    Returns:
        {
            "answer": "Based on your writing...",
            "sources": [
                {"node_id": 123, "similarity": 0.89, "excerpt": "..."}
            ],
            "tokens_used": 1234
        }
    """
    data = request.get_json()
    query = data.get("query")
    model_id = data.get("model", "claude-sonnet-4.5")
    context_size = data.get("context_size", 5)

    if not query:
        return jsonify({"error": "Query is required"}), 400

    # Step 1: Semantic search to retrieve relevant nodes
    relevant_nodes = semantic_search_internal(
        user_id=current_user.id,
        query=query,
        limit=context_size
    )

    if not relevant_nodes:
        return jsonify({
            "answer": "I couldn't find any relevant entries in your archive for this question.",
            "sources": [],
            "tokens_used": 0
        }), 200

    # Step 2: Build context from retrieved nodes
    context_parts = []
    for i, node in enumerate(relevant_nodes, 1):
        context_parts.append(f"""
[Entry {i}] (Created: {node['created_at']}, Similarity: {node['similarity']:.2f})
{node['content']}
""")

    context = "\n---\n".join(context_parts)

    # Step 3: Build prompt
    system_prompt = f"""You are helping {current_user.username} explore their writing archive.
You have access to their past journal entries. Answer their question based on the provided context.

Be conversational and insightful. Point out patterns, themes, and connections.
Cite specific entries when relevant (e.g., "In Entry 2, you wrote...").

If the context doesn't contain relevant information, say so clearly."""

    user_prompt = f"""Context from my archive:
{context}

---

My question: {query}"""

    messages = [
        {
            "role": "user",
            "content": [{"type": "text", "text": user_prompt}]
        }
    ]

    # Add system message if using Claude (OpenAI handles differently)
    if "claude" in model_id:
        # Claude uses system parameter separately
        pass

    # Step 4: Call LLM
    api_keys = {
        "openai": current_app.config["OPENAI_API_KEY"],
        "anthropic": current_app.config["ANTHROPIC_API_KEY"]
    }

    response = LLMProvider.get_completion(
        model_id,
        messages,
        api_keys,
        system_prompt=system_prompt
    )

    # Step 5: Return answer with sources
    return jsonify({
        "answer": response["content"],
        "sources": [
            {
                "node_id": node["node_id"],
                "similarity": node["similarity"],
                "excerpt": node["content"][:200] + "...",
                "created_at": node["created_at"]
            }
            for node in relevant_nodes
        ],
        "tokens_used": response["total_tokens"],
        "model_used": model_id
    }), 200
```

### Phase 1 Summary: Implementation Checklist

**Database:**
- [ ] Add pgvector extension to PostgreSQL
- [ ] Add `embedding` column to `node` table (migration)
- [ ] Create vector index

**Backend:**
- [ ] Create `backend/tasks/embeddings.py` (embed nodes)
- [ ] Create `backend/routes/search.py` (semantic search)
- [ ] Create `backend/routes/chat.py` (RAG chat)
- [ ] Add embedding generation hook to node creation/update
- [ ] Create backfill command to embed existing nodes

**Frontend:**
- [ ] Create chat interface component
- [ ] Add search results display
- [ ] Show source citations with links to nodes
- [ ] Add "Ask about your archive" button in dashboard

**Time Estimate:** 1-2 weeks

---

## Phase 2: MemeOS Integration (Personalized Recommendations)

### Goal
Use your Write or Perish profile + current writing context to recommend relevant content from MemeOS (bookmarks, Twitter saves).

**Use Cases:**
- You're writing about "habit formation" → MemeOS suggests relevant bookmarked articles
- After journaling about "burnout" → Get spaced repetition cards on related topics
- Profile indicates interest in "AI safety" → Surface older bookmarks that are due for review

### Architecture Components

#### 2.1 Understanding MemeOS

**Assumptions (please validate):**
- MemeOS stores bookmarks/tweets with metadata (title, content, tags, source)
- Has spaced repetition algorithm (tracks when content should resurface)
- Likely has a database or API you can query
- May have its own embeddings or categorization

**Questions to Answer:**
1. Does MemeOS have an API? If not, does it share a database with Write or Perish?
2. What fields exist on bookmarks? (title, url, content, tags, created_at, last_reviewed, etc.)
3. Does MemeOS already have embeddings/search, or is it purely tag/metadata based?
4. What's the spaced repetition data model? (review_count, next_review_date, etc.)

#### 2.2 Integration Architecture

**Option A: Shared Database**
If both apps use same PostgreSQL instance:

```python
# Write or Perish can directly query MemeOS tables
from backend.extensions import db

class MemeOSBookmark(db.Model):
    __tablename__ = "memeos_bookmarks"  # Or whatever MemeOS uses
    __bind_key__ = "memeos"  # If separate database

    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Integer)
    title = db.Column(db.String)
    url = db.Column(db.String)
    content = db.Column(db.Text)
    tags = db.Column(db.ARRAY(db.String))
    created_at = db.Column(db.DateTime)
    # Spaced repetition fields
    review_count = db.Column(db.Integer, default=0)
    next_review_date = db.Column(db.DateTime)
```

**Option B: API Integration**
If MemeOS is separate service:

```python
# backend/services/memeos_client.py

import requests
from typing import List, Dict

class MemeOSClient:
    def __init__(self, base_url: str, api_key: str):
        self.base_url = base_url
        self.api_key = api_key

    def get_bookmarks(
        self,
        user_id: int,
        tags: List[str] = None,
        due_for_review: bool = False
    ) -> List[Dict]:
        """Fetch bookmarks from MemeOS."""
        params = {"user_id": user_id}
        if tags:
            params["tags"] = ",".join(tags)
        if due_for_review:
            params["due_for_review"] = True

        response = requests.get(
            f"{self.base_url}/api/bookmarks",
            params=params,
            headers={"Authorization": f"Bearer {self.api_key}"}
        )
        return response.json()["bookmarks"]

    def search_bookmarks(self, user_id: int, query: str) -> List[Dict]:
        """Semantic search in MemeOS bookmarks."""
        response = requests.post(
            f"{self.base_url}/api/search",
            json={"user_id": user_id, "query": query},
            headers={"Authorization": f"Bearer {self.api_key}"}
        )
        return response.json()["results"]
```

#### 2.3 Context-Aware Recommendation Engine

**Inputs:**
1. **User Profile** - From `UserProfile` model (interests, themes, style)
2. **Current Node** - What they're writing about right now
3. **Thread Context** - Parent nodes in current thread
4. **Recent Writing** - Last N nodes created

**Output:**
- Ranked list of MemeOS bookmarks relevant to current context

**Implementation Strategy:**

```python
# backend/services/recommendation_engine.py

from typing import List, Dict
from backend.models import Node, UserProfile
from backend.services.memeos_client import MemeOSClient
from openai import OpenAI

class RecommendationEngine:
    def __init__(self, openai_client: OpenAI, memeos_client: MemeOSClient):
        self.openai = openai_client
        self.memeos = memeos_client

    def get_recommendations(
        self,
        user_id: int,
        current_node_id: int = None,
        limit: int = 5
    ) -> List[Dict]:
        """
        Get personalized MemeOS recommendations based on Write or Perish context.

        Algorithm:
        1. Extract context from current writing
        2. Use profile for personalization
        3. Query MemeOS with semantic search
        4. Apply spaced repetition filtering
        5. Rank by relevance + urgency
        """

        # Step 1: Gather context
        profile = UserProfile.query.filter_by(user_id=user_id).order_by(
            UserProfile.created_at.desc()
        ).first()

        context_text = ""

        # Add profile for personalization
        if profile:
            context_text += f"User Profile:\n{profile.content}\n\n"

        # Add current node if specified
        if current_node_id:
            node = Node.query.get(current_node_id)
            if node:
                context_text += f"Current Writing:\n{node.content}\n\n"

                # Add thread context (parent nodes)
                thread_nodes = self._get_thread_context(node)
                if thread_nodes:
                    context_text += "Thread Context:\n"
                    for n in thread_nodes:
                        context_text += f"- {n.content[:200]}\n"

        # Step 2: Extract key themes/topics using LLM
        themes = self._extract_themes(context_text)

        # Step 3: Search MemeOS bookmarks
        candidates = []

        # Semantic search with extracted themes
        for theme in themes:
            results = self.memeos.search_bookmarks(user_id, theme)
            candidates.extend(results)

        # Also get bookmarks due for review
        due_bookmarks = self.memeos.get_bookmarks(
            user_id,
            due_for_review=True
        )
        candidates.extend(due_bookmarks)

        # Step 4: Deduplicate and rank
        unique_candidates = self._deduplicate(candidates)
        ranked = self._rank_by_relevance_and_urgency(
            unique_candidates,
            context_text,
            themes
        )

        return ranked[:limit]

    def _extract_themes(self, context_text: str) -> List[str]:
        """Use LLM to extract key themes from context."""
        response = self.openai.chat.completions.create(
            model="gpt-4o-mini",  # Fast and cheap for this
            messages=[
                {
                    "role": "system",
                    "content": "Extract 3-5 key themes or topics from the text. Return as comma-separated list."
                },
                {
                    "role": "user",
                    "content": context_text[:4000]  # Limit input
                }
            ],
            temperature=0.3
        )

        themes_str = response.choices[0].message.content
        return [t.strip() for t in themes_str.split(",")]

    def _get_thread_context(self, node: Node, depth: int = 3) -> List[Node]:
        """Get parent nodes up to depth."""
        context = []
        current = node.parent
        while current and depth > 0:
            context.append(current)
            current = current.parent
            depth -= 1
        return list(reversed(context))

    def _deduplicate(self, bookmarks: List[Dict]) -> List[Dict]:
        """Remove duplicate bookmarks by ID."""
        seen = set()
        unique = []
        for b in bookmarks:
            if b["id"] not in seen:
                seen.add(b["id"])
                unique.append(b)
        return unique

    def _rank_by_relevance_and_urgency(
        self,
        bookmarks: List[Dict],
        context_text: str,
        themes: List[str]
    ) -> List[Dict]:
        """
        Rank bookmarks by:
        1. Semantic relevance to current context
        2. Spaced repetition urgency (overdue items ranked higher)
        """
        # Generate embedding for context
        context_embedding = self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=context_text[:8000]
        ).data[0].embedding

        scored = []
        for bookmark in bookmarks:
            # Semantic relevance (assuming MemeOS has embeddings)
            if "embedding" in bookmark:
                relevance = self._cosine_similarity(
                    context_embedding,
                    bookmark["embedding"]
                )
            else:
                # Fallback: keyword matching
                relevance = self._keyword_relevance(bookmark, themes)

            # Urgency (how overdue for review)
            from datetime import datetime
            if bookmark.get("next_review_date"):
                days_overdue = (
                    datetime.utcnow() - bookmark["next_review_date"]
                ).days
                urgency = max(0, days_overdue) / 30  # Normalize to 0-1
            else:
                urgency = 0

            # Combined score (70% relevance, 30% urgency)
            score = 0.7 * relevance + 0.3 * urgency

            scored.append({
                **bookmark,
                "relevance": relevance,
                "urgency": urgency,
                "score": score
            })

        # Sort by score descending
        scored.sort(key=lambda x: x["score"], reverse=True)
        return scored

    def _cosine_similarity(self, vec_a, vec_b):
        """Calculate cosine similarity between two vectors."""
        import numpy as np
        return np.dot(vec_a, vec_b) / (
            np.linalg.norm(vec_a) * np.linalg.norm(vec_b)
        )

    def _keyword_relevance(self, bookmark: Dict, themes: List[str]) -> float:
        """Fallback: keyword-based relevance scoring."""
        text = f"{bookmark.get('title', '')} {bookmark.get('content', '')}"
        text_lower = text.lower()

        matches = sum(1 for theme in themes if theme.lower() in text_lower)
        return matches / len(themes) if themes else 0
```

#### 2.4 API Endpoint

```python
# backend/routes/recommendations.py

from flask import Blueprint, request, jsonify
from flask_login import login_required, current_user
from backend.services.recommendation_engine import RecommendationEngine
from backend.services.memeos_client import MemeOSClient
from openai import OpenAI

recommendations_bp = Blueprint("recommendations", __name__)

@recommendations_bp.route("/memeos", methods=["GET"])
@login_required
def get_memeos_recommendations():
    """
    Get personalized MemeOS recommendations.

    Query params:
        - current_node_id: Optional node ID for context
        - limit: Number of recommendations (default 5)

    Returns:
        {
            "recommendations": [
                {
                    "id": 123,
                    "title": "How to Build Better Habits",
                    "url": "https://...",
                    "relevance": 0.87,
                    "urgency": 0.45,
                    "score": 0.74,
                    "next_review_date": "2025-11-20T10:00:00Z"
                }
            ]
        }
    """
    current_node_id = request.args.get("current_node_id", type=int)
    limit = request.args.get("limit", default=5, type=int)

    # Initialize clients
    openai_client = OpenAI(api_key=current_app.config["OPENAI_API_KEY"])
    memeos_client = MemeOSClient(
        base_url=current_app.config["MEMEOS_BASE_URL"],
        api_key=current_app.config["MEMEOS_API_KEY"]
    )

    # Get recommendations
    engine = RecommendationEngine(openai_client, memeos_client)
    recommendations = engine.get_recommendations(
        user_id=current_user.id,
        current_node_id=current_node_id,
        limit=limit
    )

    return jsonify({
        "recommendations": recommendations
    }), 200
```

#### 2.5 Frontend Integration

**UI Placement Options:**

1. **Sidebar Widget** (Node Detail Page)
   ```jsx
   // frontend/src/components/MemeOSRecommendations.js

   function MemeOSRecommendations({ nodeId }) {
     const [recommendations, setRecommendations] = useState([]);
     const [loading, setLoading] = useState(true);

     useEffect(() => {
       fetch(`/api/recommendations/memeos?current_node_id=${nodeId}`)
         .then(res => res.json())
         .then(data => {
           setRecommendations(data.recommendations);
           setLoading(false);
         });
     }, [nodeId]);

     if (loading) return <div>Loading recommendations...</div>;

     return (
       <div className="memeos-recommendations">
         <h3>Related from MemeOS</h3>
         {recommendations.map(rec => (
           <div key={rec.id} className="recommendation-card">
             <a href={rec.url} target="_blank" rel="noopener">
               {rec.title}
             </a>
             <div className="meta">
               Relevance: {(rec.relevance * 100).toFixed(0)}%
               {rec.urgency > 0 && ` • Due for review`}
             </div>
           </div>
         ))}
       </div>
     );
   }
   ```

2. **Dashboard Section**
   - Show recommendations based on recent writing
   - "Continue your learning" section

3. **Notification System**
   - "You just wrote about X, check out these related bookmarks"

### Phase 2 Summary: Implementation Checklist

**Backend:**
- [ ] Decide on MemeOS integration method (shared DB vs API)
- [ ] Create `backend/services/memeos_client.py` (if API)
- [ ] Create `backend/services/recommendation_engine.py`
- [ ] Create `backend/routes/recommendations.py`
- [ ] Add MemeOS config variables (base URL, API key)

**MemeOS (if building API):**
- [ ] Create API endpoints for bookmark search
- [ ] Add filtering by tags, due dates
- [ ] Consider adding embeddings to bookmarks for better search

**Frontend:**
- [ ] Create `MemeOSRecommendations` component
- [ ] Add to NodeDetail page sidebar
- [ ] Add to Dashboard as section
- [ ] Style recommendation cards

**Time Estimate:** 1-2 weeks (depending on MemeOS API availability)

---

## Phase 3: Unified Intelligence (Bringing It All Together)

### Goal
Create a seamless loop where your writing informs content consumption and vice versa.

**The Flywheel:**
1. You write in Write or Perish
2. Profile is generated/updated from your writing
3. MemeOS surfaces relevant content based on profile + current context
4. You consume content in MemeOS (spaced repetition)
5. Insights from reading spark new writing
6. Cycle repeats

### Advanced Features

#### 3.1 Bidirectional Sync
- Writing about topic X → Tag MemeOS content with X
- Reviewing bookmark Y in MemeOS → Create node in Write or Perish with reflections

#### 3.2 Auto-Generated Prompts
When you finish a writing session, suggest:
- "You just wrote about [topic], here are some questions to explore further"
- "Related bookmarks from MemeOS you haven't reviewed in 30 days"

#### 3.3 Trend Analysis
- "You've written about 'habit formation' 5 times this month, but haven't reviewed your bookmarks on this topic"
- "Your writing shows increasing interest in X, but MemeOS has no content on X - should we find some?"

#### 3.4 Multi-Modal Profile
Combine:
- Writing style from Write or Perish
- Consumption patterns from MemeOS
- Time-series analysis (interests evolving over time)

Generate: "Holistic knowledge graph" of your intellectual life

---

## Implementation Priority Recommendations

### If Building for Personal Use (Recommended)

**Month 1: RAG Foundation**
- Week 1-2: Add pgvector, embeddings, semantic search
- Week 3-4: Build chat interface

**Month 2: MemeOS Integration**
- Week 1-2: Set up integration (API or DB)
- Week 3-4: Build recommendation engine

**Month 3: Polish & Advanced Features**
- Week 1-2: Bidirectional sync
- Week 3-4: Auto-prompts, trend analysis

### Quick Wins (This Week)

1. **Install pgvector** (30 minutes)
   ```bash
   # On your PostgreSQL server
   sudo apt-get install postgresql-15-pgvector

   # In psql
   CREATE EXTENSION vector;
   ```

2. **Embed 10 nodes manually** (1 hour)
   - Proof of concept for embeddings
   - Test semantic search query
   - Validate approach before full build

3. **Mock recommendation endpoint** (2 hours)
   - Return static recommendations
   - Build frontend component
   - Test UI/UX before building real engine

---

## Technical Considerations

### Cost Analysis

**RAG (Chat with Archive):**
- Embedding generation: ~$0.02 per 1M tokens
  - Estimate: 100 nodes × 1000 tokens = 100k tokens = $0.002
  - One-time backfill + ongoing per-node (negligible)
- Chat queries: Standard LLM costs
  - Estimate: 5 retrieved nodes × 500 tokens = 2.5k context
  - + Your query ~100 tokens = ~2.6k input tokens per chat
  - Claude Sonnet 4.5: $3/MTok input = $0.0078 per chat
  - Very affordable for personal use

**MemeOS Recommendations:**
- Theme extraction: GPT-4o-mini (~$0.15/MTok)
  - ~1k tokens per recommendation request = $0.00015
- Embedding for context: Same as above ($0.02/MTok)
  - ~2k tokens = $0.00004
- Total per recommendation request: ~$0.0002 (essentially free)

### Performance Considerations

**Vector Search:**
- pgvector with 1000 nodes: <10ms query time
- Scales to ~1M vectors before needing optimization
- Your personal use case: will never hit limits

**Recommendation Engine:**
- Most expensive: LLM call for theme extraction (~500ms)
- MemeOS query: depends on their implementation
- Target: <2s end-to-end for recommendations

### Data Privacy

**If MemeOS is also your project:**
- Shared user IDs, straightforward integration
- Both systems under your control

**If MemeOS might be shared/public:**
- Consider: Do you want Write or Perish content to influence public MemeOS?
- Recommendation: Keep profile private, only use for personal recommendations

---

## Questions to Answer Before Starting

1. **MemeOS Architecture:**
   - Is MemeOS a separate app or module?
   - Does it have an API?
   - Can Write or Perish query its database directly?
   - What's the data model for bookmarks?

2. **Embedding Strategy:**
   - Should MemeOS bookmarks also have embeddings?
   - Who generates them (Write or Perish or MemeOS)?
   - Shared embedding model or separate?

3. **Recommendation Frequency:**
   - Real-time as you write?
   - On-demand when you click a button?
   - Daily digest?

4. **Spaced Repetition Integration:**
   - Should Write or Perish writing update spaced repetition schedules?
   - If you write about a topic, should MemeOS mark related bookmarks as "reinforced"?

---

## Next Steps

### Immediate (This Week)
1. Answer the questions above about MemeOS architecture
2. Install pgvector extension on your PostgreSQL
3. Create a test branch for RAG experimentation
4. Manually embed 10 nodes and test semantic search

### Short-Term (Month 1)
1. Complete Phase 1 (RAG Foundation)
2. Build chat interface
3. Use it daily to validate value

### Medium-Term (Month 2-3)
1. Phase 2 (MemeOS Integration) once RAG is stable
2. Build recommendation engine
3. Test bidirectional workflows

### Long-Term (Month 4+)
1. Advanced features (auto-prompts, trend analysis)
2. Multi-modal profile
3. Knowledge graph visualization

---

## Conclusion

You're building toward something unique: **a closed-loop learning system** that connects consumption (MemeOS) with creation (Write or Perish).

The architecture above provides:
- **Semantic search** over your writing (RAG)
- **Context-aware recommendations** from your bookmarks
- **Spaced repetition integration** for knowledge retention
- **Personal profile** as the unifying intelligence layer

Start with Phase 1 (RAG) because:
1. It's immediately useful for you personally
2. It's foundational for Phase 2 (you'll reuse embeddings/search)
3. It's technically simpler (no cross-system integration)
4. It proves the value before investing in MemeOS connection

**The key insight:** Your personal profile + current writing context = the perfect filter for content recommendations. This is the unique value prop that generic recommendation systems can't provide.

Build it, use it, iterate. The flywheel will accelerate as you close the loop between reading and writing.
